<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="./images/icon.png">
<link rel="stylesheet" href="main.css" type="text/css" />
<link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
<!--- <title></title> --->
<title>Hengyue's Cabin (梁恒岳的小木屋)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<div id="main-container">
<div id="header-container">
<div id="header">
<div id="header-icon-text-container">
<div id="header-icon-container" >
<a href="index.html"><img src="./images/profile2.png" alt="" style="width: 100%; height: 100%; position: center; padding:0px; margin: 0px;"></a>
</div>
<div id="header-text-container">
<a href="index.html">Hengyue's Cabin (梁恒岳的小木屋)</a>
</div>
</div>
<div id="main">
<button class="openbtn" onclick="openNav()">☰</button>
</div>
</div>
</div>
<div id="layout">
<div id="layout-menu-container">
<div id="layout-menu">
<div class="menu-item"><a href="javascript:void(0)" class="closebtn" onclick="closeNav()">×</a></div>
<div class="menu-item"><a href="index.html">Personal&nbsp;Profile</a></div>
<div class="menu-item"><a href="index_cn.html" class="current">个人简介(中文版)</a></div>
<div class="menu-item"><a href="post1.html">Blog&nbsp;Post&nbsp;Page</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="Work.html">Work&nbsp;Experience</a></div>
</div> <!-- <div id="layout-menu"> -->
</div> <!-- <div id="layout-menu-container"> -->
<div id="layout-content-container">
<div id="layout-content">
<div id="text-img-container"><div id="img-container">
<img src="./images/header.png" alt="profile pic" width="100%" /></div>
<div id="text-container"></div></div>
<div id="text-img-container"><div id="img-container">
<img src="./images/profile.png" alt="profile pic" width="200px" height="200px" /></div>
<div id="text-container"><h2>梁恒岳 <br /></h2>
<h3>在读 Ph.D. 第五年 （预计24年秋季毕业，有合适的工作岗位请不吝联系我!）<br /></h3>
<h3><a href="https://cse.umn.edu/ece/" target=&ldquo;blank&rdquo;>Department of Electrical and Computer Engineering</a> <br /></h3>
<h3><a href="https://twin-cities.umn.edu/" target=&ldquo;blank&rdquo;>明尼苏达大学双城校区(University of Minnesota, Twin Cities)</a> <br /></h3>
<h3>Email: liang656 (at) umn (dot) edu <br /></h3>
<p><br /></p>
<h3>其他相关链接: <br /></h3>
<h3>| <a href="https://scholar.google.com/citations?user=aWVo5AEAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a> | <a href="./CV/My_CV_202209.pdf" target=&ldquo;blank&rdquo;>CV</a> | <a href="https://www.linkedin.com/in/lianghengyue/" target=&ldquo;blank&rdquo;>LinkedIn</a> | <a href="https://github.com/HengyueL" target=&ldquo;blank&rdquo;>Github</a> | <br /></h3>
</div></div>
<h2>个人简介</h2>
<p>欢迎光临我的主页 ;) <br /> 
<br />
我是一名在明尼苏达大学（明大）双城校区的第五年的在读Ph.D.学生。 很荣幸的为您介绍我目前的导师， <a href="https://sunju.org/" target=&ldquo;blank&rdquo;>孙举教授</a>.<br />
<br />
一切顺利的话，我将于24年秋季毕业，若有合适的工作机会请联系我！
<br />
在来到明大之前, 我于瑞典<a href="https://www.chalmers.se/sv/Sidor/default.aspx" target=&ldquo;blank&rdquo;>查尔姆斯理工大学</a>电气工程学院获得硕士研究生学位； 本科则是就读于<a href="https://en.sjtu.edu.cn/" target=&ldquo;blank&rdquo;>上海交通大学</a>电子信息与电气工程学院， 自动化专业. <br />
<br />
我所在的明大实验室<a href="https://glovex.umn.edu/" target=&ldquo;blank&rdquo;>GLOVEX</a>以实现 “实用的， 可靠的， 可部署的医疗AI” 为主要研究导向。 其中， 我的研究方向主要集中与如何让深度学习模型更鲁棒（对抗鲁棒性，自然鲁棒性）和如何让深度学习模型做出更可靠的预测（选择性分类）, 其中重点关注计算机视觉中 的问题。 
在一项与明大医学院合作的开拓性的AI医学研究 “基于计算机视觉的图雷特综合症自动定量方法”， 我为其中计算机视觉算法及视频分析算法的主要贡献者。该项目目前已获得美国国立卫生研究院（NIH）的为期五年的科研资助（总计约350万美金）。<br />
<br />
此外，我也有3D重建和3D模型重建的经验（亚马逊实习项目）以及机器人，系统控制， 强化学习的相关经验（本科及研究生，以及加入孙举老师实验室前的学术背景）。</p>
<h2>最新动态</h2>
<p><b>[ June 2023 ]</b> I returned to Amazon again as an Applied Scientist Intern, working on reconstucting 3D objects and generating 3D assets for realistic talking avatars.<br /></p>
<p><b>[ May 2023 ]</b> I submitted a paper &ldquo;Toward Effective Post-Training Selective Classification for High-Stakes Applications&rdquo; to NeurIPS 2023. Stay tuned for the preprint release :) <br /></p>
<p><b>[ May 2023 ]</b> I presented a poster about the effort of our group in seeking trustworthy AI at <a href="https://midwest-ml.org/2023/" target=&ldquo;blank&rdquo;>Midwest Machine Learning Symposium 2023</a>. <br /></p>
<p><b>[ May 2023 ]</b> The preliminary study &ldquo;Automated Quantification of Eye Tics using Computer Vision and Deep Learning Techniques&rdquo; are now under review for &ldquo;Movement Disorders&rdquo;, the official Journal of Movement Disorder Society
(MDS). <br /></p>
<p><b>[ May 2023 ]</b> Prof <a href="https://sunju.org/" target=&ldquo;blank&rdquo;>Ju Sun</a>, with <a href="https://med.umn.edu/bio/christine-conelea" target=&ldquo;blank&rdquo;>Prof. Christine Conelea</a> and <a href="https://med.umn.edu/bio/kelvin-lim" target=&ldquo;blank&rdquo;>Prof. Kelvin Lim</a> of UMN Medical School, receives NIH NINDS research grant  (~ 3.5 M US dollars in total for the next 5 years) to develop novel video analysis tools to help doctors expedite the diagnosis and treatment of Tourette syndrome, 
where I am the main contributor of the video analysis and tic motion detection algorithms in the preliminary study of the grant proposal.<br /></p>
<p><b>[ Apr 2023 ]</b> I served as an assitant session chair in hosting <a href="https://www.siam.org/conferences/cm/conference/sdm23" target=&ldquo;blank&rdquo;>SDM23</a> coference.<br /></p>
<p><b>[ Mar 2023 ]</b> Our paper &ldquo;Optimization and optimizers for adversarial robustness&rdquo; is released on arXiv, where we extensively discussed our (pessimistic) view on current adversarial robsutness studies. |&nbsp;<a href="https://arxiv.org/abs/2303.13401" target=&ldquo;blank&rdquo;>Paper</a>&nbsp;|<br /></p>
<p><b>[ Feb 2023 ]</b> Our paper &ldquo;Implications of Solution Patterns on Adversarial Robustness&rdquo; is accepted to CVPR 2023 (workshop of adversarial machine learning). | <a href="https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Liang_Implications_of_Solution_Patterns_on_Adversarial_Robustness_CVPRW_2023_paper.html" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ Oct 2022 ]</b> Our paper &ldquo;Optimization for Robustness Evaluation beyond ℓp Metrics&rdquo; is accepted to ICASSP 2023. <br /> | <a href="https://ieeexplore.ieee.org/abstract/document/10095871" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ Dec 2021 ]</b> Our paper &ldquo;Early Stopping for Deep Image Prior&rdquo; is submitted to Conference on Computer Vision and Pattern Recognition (CVPR) 2022. | <a href="https://arxiv.org/abs/2112.06074" target=&ldquo;blank&rdquo;>Preprint</a> | <br /></p>
<p><b>[ Oct 2021 ]</b> Our paper &ldquo;Self-Validation: Early Stopping for Single-Instance Deep Generative Priors&rdquo; is accepted to British Machine Vision Conference (BMVC) 2021! | <a href="https://arxiv.org/abs/2110.12271" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ June - Sep 2021 ]</b> I worked at Amazon as an Applied Scientist Intern, conducting a research project on generating realistic head motions for virtual animated avatar based on speech audio only. <br /></p>
<p><b>[ June 2021 ]</b> A study on transfer learning for medical image classification is available. Our study shows that transfer learning should probably be performed on truncated deep models, rather than full deep models which are conventionally used. &nbsp;&nbsp; | <a href="https://sun-umn.github.io/Transfer-Learning-in-Medical-Imaging/" target=&ldquo;blank&rdquo;>Project Blog</a> | <a href="https://arxiv.org/abs/2106.05152/" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ June 2021 ]</b> The preprint paper of a deployed AI powered diagnose assistant project for COVID-19 by our group is released on medRxiv. &nbsp;&nbsp; | <a href="https://www.medrxiv.org/content/10.1101/2021.06.04.21258316v1/" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ January 2021 ]</b> Our paper &ldquo;Learning Visual Affordances with Target-Orientated Deep Q-Network to Grasp Objectsby Harnessing Environmental Fixtures&rdquo; was accepted and published at IEEE International Conference on Robotics and Automation (ICRA) 2021, Xi'an &nbsp;&nbsp; | <a href="https://sites.google.com/umn.edu/ki-dqn/" target=&ldquo;blank&rdquo;>Project Page</a> | <a href="https://arxiv.org/abs/1910.03781/" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ January 2021 ]</b> Our paper &ldquo;Attribute-Based Robotic Grasping with One-Grasp Adaptation&rdquo; was accepted and published at IEEE International Conference on Robotics and Automation (ICRA) 2021, Xi'an &nbsp;&nbsp; | <a href="https://sites.google.com/umn.edu/attributes-grasping/" target=&ldquo;blank&rdquo;>Project Page</a> | <a href="https://arxiv.org/pdf/2104.02271.pdf" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ January 2020 ]</b> Our paper &ldquo;A Deep Learning Approach to Grasping the Invisible&rdquo; was accepted and published at IEEE International Conference on Robotics and Automation (ICRA) 2020, Paris &nbsp;&nbsp; | <a href="https://sites.google.com/umn.edu/grasping-invisible/" target=&ldquo;blank&rdquo;>Project Page</a> | <a href="https://arxiv.org/pdf/1909.04840.pdf" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
</div> <!-- <div id="layout-content"> -->
<div id="footer-container">
<div id="footer">
<div id="footer-text">
Last edited  on June 17<sup>th</sup> 2023  10:13PM (Time Zone: PDT). </br>
Powered by <a href="https://github.com/szl2/jemdoc-new-design" target="blank">jemdoc + new design</a>.
</div> <!-- <div id="footer-text"> -->
</div> <!-- <div id="footer"> -->
</div> <!-- <div id="footer-container"> -->
</div> <!-- <div id="layout-content-container"> -->
</div> <!--- <div id="layout"> --->
</div> <!--- <div id="main-container"> --->
<script>
function openNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "280px";
        document.getElementById("layout-content-container").style.marginLeft = "280.8px";
        document.getElementById("layout-content-container").style.position = "fixed";
    }
}
function closeNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "0";
        document.getElementById("layout-content-container").style.position = "static";
        document.getElementById("layout-content-container").style.marginLeft = "0px";
        setInterval(
            function(){ location.reload() },
            500
        );
    }
}
</script>
</body>
</html>
