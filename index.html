<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="icon" href="./images/icon.png">
<link rel="stylesheet" href="main.css" type="text/css" />
<link rel="stylesheet" href="font-awesome/css/font-awesome.min.css">
<!--- <title></title> --->
<title>Hengyue's Cabin (梁恒岳的小木屋)</title>
<!-- MathJax -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async>
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
	  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
<!-- End MathJax -->
</head>
<body>
<div id="main-container">
<div id="header-container">
<div id="header">
<div id="header-icon-text-container">
<div id="header-icon-container" >
<a href="index.html"><img src="./images/profile2.png" alt="" style="width: 100%; height: 100%; position: center; padding:0px; margin: 0px;"></a>
</div>
<div id="header-text-container">
<a href="index.html">Hengyue's Cabin (梁恒岳的小木屋)</a>
</div>
</div>
<div id="main">
<button class="openbtn" onclick="openNav()">☰</button>
</div>
</div>
</div>
<div id="layout">
<div id="layout-menu-container">
<div id="layout-menu">
<div class="menu-item"><a href="javascript:void(0)" class="closebtn" onclick="closeNav()">×</a></div>
<div class="menu-item"><a href="index.html" class="current">Personal&nbsp;Profile</a></div>
<div class="menu-item"><a href="index_cn.html">个人简介(中文版)</a></div>
<div class="menu-item"><a href="publication.html">Publications</a></div>
<div class="menu-item"><a href="Work.html">Work&nbsp;Experience</a></div>
</div> <!-- <div id="layout-menu"> -->
</div> <!-- <div id="layout-menu-container"> -->
<div id="layout-content-container">
<div id="layout-content">
<div id="text-img-container"><div id="img-container">
<img src="./images/header.png" alt="profile pic" width="100%" /></div>
<div id="text-container"></div></div>
<div id="text-img-container"><div id="img-container">
<img src="./images/profile.png" alt="profile pic" width="200px" height="200px" /></div>
<div id="text-container"><h2>Hengyue Liang (梁恒岳) <br /></h2>
<h3>Ph.D. Candidate (Open to work as soon as Fall 2024)<br /></h3>
<h3><a href="https://cse.umn.edu/ece/" target=&ldquo;blank&rdquo;>Department of Electrical and Computer Engineering</a> <br /></h3>
<h3><a href="https://twin-cities.umn.edu/" target=&ldquo;blank&rdquo;>University of Minnesota, Twin Cities&nbsp;(UMN)</a> <br /></h3>
<h3>Email: liang656 (at) umn (dot) edu <br /></h3>
<p><br /></p>
<h3>Other referece: <br /></h3>
<h3>| <a href="https://scholar.google.com/citations?user=aWVo5AEAAAAJ&amp;hl=en" target=&ldquo;blank&rdquo;>Google Scholar</a> | <a href="./CV/My_CV_202306.pdf" target=&ldquo;blank&rdquo;>CV</a> | <a href="https://www.linkedin.com/in/lianghengyue/" target=&ldquo;blank&rdquo;>LinkedIn</a> | <a href="https://github.com/HengyueL" target=&ldquo;blank&rdquo;>Github</a> | <br /></h3>
<p><br /></p>
<h3>中文版简介请移步<a href="./index_cn.html" target=&ldquo;blank&rdquo;>这里</a></h3>
</div></div>
<h2>Biography</h2>
<p>Welcome! Glad to see you visit ;) <br /> 
<br />
My name is Hengyue, currently a 5th-year Ph.D. student in <a href="https://cse.umn.edu/ece/" target=&ldquo;blank&rdquo;>Department of Electrical Engineering</a>, <a href="https://twin-cities.umn.edu/" target=&ldquo;blank&rdquo;>University of Minnesota, Twin Cities, USA</a>. I'm grateful to be advised by <a href="https://sunju.org/" target=&ldquo;blank&rdquo;>Prof. Ju Sun</a>.<br />
<br />
If things goes smoothly, I would have been graduating by Fall 2024. Please contact me if you have job opennings and are interested in knowing me more :). <br />
<br />
Before studying at UMN, I earned a Master's Degree in Electrical Engineering at <a href="https://www.chalmers.se/sv/Sidor/default.aspx" target=&ldquo;blank&rdquo;>Chalmers University of Technology, Sweden</a>, and a Bachelor's Degree in Electrical Engineering at <a href="https://en.sjtu.edu.cn/" target=&ldquo;blank&rdquo;>Shanghai Jiao Tong University, China</a>. <br />
<br />
The main theme of our group <a href="https://glovex.umn.edu/" target=&ldquo;blank&rdquo;>GLOVEX</a> is to study any possible approaches to achieve &ldquo;pratical, reliable and deployable medical AI systems&rdquo;. Aligned with it, my main rearch interest lies in how to make deep learning models more robust (adversarial and natural robustness) and how to determin what are more reliable and trustworthy decisions of deep learning models (selective classifications), mostly related to computer vision. 
I am also one of the main contributor (responsible for computer vision and video analysis algorithm) in a pioneer study of &ldquo;Automated Quantification of Tics (Tourette Syndrome) using computer vision techniques&rdquo;, a collaborated project (funded by <a href="https://www.ninds.nih.gov/" target=&ldquo;blank&rdquo;>NIH&nbsp;NINDs</a>) of our group with <a href="https://med.umn.edu/bio/christine-conelea" target=&ldquo;blank&rdquo;>Prof. Christine Conelea</a> and <a href="https://med.umn.edu/bio/kelvin-lim" target=&ldquo;blank&rdquo;>Prof. Kelvin Lim</a> of UMN Medical School.<br />
<br /> 
I also have experience in 3D reconstruction and 3D assets generation (Amazon internship projects). Before joing Prof Sun's research group, I also had research experience in robotics, system control and reinforcement learning (which are also my Bachelor, and Master background).</p>
<h2>News</h2>
<p><b>[ June 2023 ]</b> I returned to Amazon again as an Applied Scientist Intern, working on reconstucting 3D objects and generating 3D assets for realistic talking avatars.<br /></p>
<p><b>[ May 2023 ]</b> I submitted a paper &ldquo;Toward Effective Post-Training Selective Classification for High-Stakes Applications&rdquo; to NeurIPS 2023. Stay tuned for the preprint release :) <br /></p>
<p><b>[ May 2023 ]</b> I presented a poster about the effort of our group in seeking trustworthy AI at <a href="https://midwest-ml.org/2023/" target=&ldquo;blank&rdquo;>Midwest Machine Learning Symposium 2023</a>. <br /></p>
<p><b>[ May 2023 ]</b> The preliminary study &ldquo;Automated Quantification of Eye Tics using Computer Vision and Deep Learning Techniques&rdquo; are now under review for &ldquo;Movement Disorders&rdquo;, the official Journal of Movement Disorder Society
(MDS). <br /></p>
<p><b>[ May 2023 ]</b> Prof <a href="https://sunju.org/" target=&ldquo;blank&rdquo;>Ju Sun</a>, with <a href="https://med.umn.edu/bio/christine-conelea" target=&ldquo;blank&rdquo;>Prof. Christine Conelea</a> and <a href="https://med.umn.edu/bio/kelvin-lim" target=&ldquo;blank&rdquo;>Prof. Kelvin Lim</a> of UMN Medical School, receives NIH NINDS research grant  (~ 3.5 M US dollars in total for the next 5 years) to develop novel video analysis tools to help doctors expedite the diagnosis and treatment of Tourette syndrome, 
where I am the main contributor of the video analysis and tic motion detection algorithms in the preliminary study of the grant proposal.<br /></p>
<p><b>[ Apr 2023 ]</b> I served as an assitant session chair in hosting <a href="https://www.siam.org/conferences/cm/conference/sdm23" target=&ldquo;blank&rdquo;>SDM23</a> coference.<br /></p>
<p><b>[ Mar 2023 ]</b> Our paper &ldquo;Optimization and optimizers for adversarial robustness&rdquo; is released on arXiv, where we extensively discussed our (pessimistic) view on current adversarial robsutness studies. |&nbsp;<a href="https://arxiv.org/abs/2303.13401" target=&ldquo;blank&rdquo;>Paper</a>&nbsp;|<br /></p>
<p><b>[ Feb 2023 ]</b> Our paper &ldquo;Implications of Solution Patterns on Adversarial Robustness&rdquo; is accepted to CVPR 2023 (workshop of adversarial machine learning). | <a href="https://openaccess.thecvf.com/content/CVPR2023W/AML/html/Liang_Implications_of_Solution_Patterns_on_Adversarial_Robustness_CVPRW_2023_paper.html" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ Oct 2022 ]</b> Our paper &ldquo;Optimization for Robustness Evaluation beyond ℓp Metrics&rdquo; is accepted to ICASSP 2023. <br /> | <a href="https://ieeexplore.ieee.org/abstract/document/10095871" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ Dec 2021 ]</b> Our paper &ldquo;Early Stopping for Deep Image Prior&rdquo; is submitted to Conference on Computer Vision and Pattern Recognition (CVPR) 2022. | <a href="https://arxiv.org/abs/2112.06074" target=&ldquo;blank&rdquo;>Preprint</a> | <br /></p>
<p><b>[ Oct 2021 ]</b> Our paper &ldquo;Self-Validation: Early Stopping for Single-Instance Deep Generative Priors&rdquo; is accepted to British Machine Vision Conference (BMVC) 2021! | <a href="https://arxiv.org/abs/2110.12271" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ June - Sep 2021 ]</b> I worked at Amazon as an Applied Scientist Intern, conducting a research project on generating realistic head motions for virtual animated avatar based on speech audio only. <br /></p>
<p><b>[ June 2021 ]</b> A study on transfer learning for medical image classification is available. Our study shows that transfer learning should probably be performed on truncated deep models, rather than full deep models which are conventionally used. &nbsp;&nbsp; | <a href="https://sun-umn.github.io/Transfer-Learning-in-Medical-Imaging/" target=&ldquo;blank&rdquo;>Project Blog</a> | <a href="https://arxiv.org/abs/2106.05152/" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ June 2021 ]</b> The preprint paper of a deployed AI powered diagnose assistant project for COVID-19 by our group is released on medRxiv. &nbsp;&nbsp; | <a href="https://www.medrxiv.org/content/10.1101/2021.06.04.21258316v1/" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ January 2021 ]</b> Our paper &ldquo;Learning Visual Affordances with Target-Orientated Deep Q-Network to Grasp Objectsby Harnessing Environmental Fixtures&rdquo; was accepted and published at IEEE International Conference on Robotics and Automation (ICRA) 2021, Xi'an &nbsp;&nbsp; | <a href="https://sites.google.com/umn.edu/ki-dqn/" target=&ldquo;blank&rdquo;>Project Page</a> | <a href="https://arxiv.org/abs/1910.03781/" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ January 2021 ]</b> Our paper &ldquo;Attribute-Based Robotic Grasping with One-Grasp Adaptation&rdquo; was accepted and published at IEEE International Conference on Robotics and Automation (ICRA) 2021, Xi'an &nbsp;&nbsp; | <a href="https://sites.google.com/umn.edu/attributes-grasping/" target=&ldquo;blank&rdquo;>Project Page</a> | <a href="https://arxiv.org/pdf/2104.02271.pdf" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
<p><b>[ January 2020 ]</b> Our paper &ldquo;A Deep Learning Approach to Grasping the Invisible&rdquo; was accepted and published at IEEE International Conference on Robotics and Automation (ICRA) 2020, Paris &nbsp;&nbsp; | <a href="https://sites.google.com/umn.edu/grasping-invisible/" target=&ldquo;blank&rdquo;>Project Page</a> | <a href="https://arxiv.org/pdf/1909.04840.pdf" target=&ldquo;blank&rdquo;>Paper</a> | <br /></p>
</div> <!-- <div id="layout-content"> -->
<div id="footer-container">
<div id="footer">
<div id="footer-text">
Last edited  on June 17<sup>th</sup> 2023  10:31PM (Time Zone: PDT). </br>
Powered by <a href="https://github.com/szl2/jemdoc-new-design" target="blank">jemdoc + new design</a>.
</div> <!-- <div id="footer-text"> -->
</div> <!-- <div id="footer"> -->
</div> <!-- <div id="footer-container"> -->
</div> <!-- <div id="layout-content-container"> -->
</div> <!--- <div id="layout"> --->
</div> <!--- <div id="main-container"> --->
<script>
function openNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "280px";
        document.getElementById("layout-content-container").style.marginLeft = "280.8px";
        document.getElementById("layout-content-container").style.position = "fixed";
    }
}
function closeNav() {
    if (window.innerWidth <= 1200) {
        document.getElementById("layout-menu").style.width = "0";
        document.getElementById("layout-content-container").style.position = "static";
        document.getElementById("layout-content-container").style.marginLeft = "0px";
        setInterval(
            function(){ location.reload() },
            500
        );
    }
}
</script>
</body>
</html>
